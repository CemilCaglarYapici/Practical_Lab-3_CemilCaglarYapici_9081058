{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16767732",
   "metadata": {},
   "source": [
    "### Practical Lab 3 - Vanilla CNN and Fine-Tune VGG16 - for Dogs and Cats Classification\n",
    "\n",
    "Cemil Caglar Yapici - 9081058"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e96083e",
   "metadata": {},
   "source": [
    "This notebook demonstrates a complete workflow for classifying images of dogs vs cats using TensorFlow/Keras. We first load and explore the data, then train two models – a simple CNN from scratch and a fine-tuned VGG16 – and finally evaluate their performance.\n",
    "\n",
    "NOTE!!: I worked on 1250 cat +1250 dog photos, otherwise It crashed my pc and Too big for to upload on Github."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5f5ac4",
   "metadata": {},
   "source": [
    "### 1. Obtain the Data\n",
    "\n",
    "We assume the dataset is organized in folders Dog vs Cat/train/ and Dog vs Cat/test/. The training folder contains labeled images (cat.*.jpg and dog.*.jpg), while the test folder contains unlabeled images. We will split a portion of the training set for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa56964",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "# Set up GPU usage (fall back to CPU if GPU unavailable)\n",
    "gpu_name = tf.test.gpu_device_name()\n",
    "if gpu_name:\n",
    "    print(\"GPU detected:\", gpu_name)\n",
    "else:\n",
    "    print(\"No GPU found, using CPU\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Paths to data directories\n",
    "data_dir = 'Dog vs Cat'\n",
    "train_dir = os.path.join(data_dir, 'train')\n",
    "test_dir  = os.path.join(data_dir, 'test')\n",
    "\n",
    "# Verify counts of dog and cat images in the training set\n",
    "train_files = [f for f in os.listdir(train_dir) if f.endswith('.jpg')]\n",
    "num_dogs = sum(f.startswith('dog') for f in train_files)\n",
    "num_cats = sum(f.startswith('cat') for f in train_files)\n",
    "print(f\"Total training images: {len(train_files)} (Dogs: {num_dogs}, Cats: {num_cats})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a24b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare file paths and labels\n",
    "file_paths = [os.path.join(train_dir, fname) for fname in train_files]\n",
    "labels = [1 if fname.startswith('dog') else 0 for fname in train_files]\n",
    "\n",
    "# Shuffle and split the data\n",
    "indices = np.arange(len(file_paths))\n",
    "np.random.shuffle(indices)\n",
    "file_paths = np.array(file_paths)[indices]\n",
    "labels = np.array(labels)[indices]\n",
    "split_idx = int(0.8 * len(file_paths))\n",
    "train_paths, train_labels = file_paths[:split_idx], labels[:split_idx]\n",
    "val_paths,   val_labels   = file_paths[split_idx:], labels[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(train_paths)}, Validation samples: {len(val_paths)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc814bb6",
   "metadata": {},
   "source": [
    "### 2. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2464aa2a",
   "metadata": {},
   "source": [
    "### 2.1 Display Sample Images\n",
    "We visualize a few random training images with their labels to ensure data looks correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a206f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "# Show a few random training images with labels\n",
    "plt.figure(figsize=(10,4))\n",
    "for i in range(6):\n",
    "    idx = np.random.randint(len(train_paths))\n",
    "    img = mpimg.imread(train_paths[idx])\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.title(\"Dog\" if train_labels[idx]==1 else \"Cat\")\n",
    "    plt.axis('off')\n",
    "plt.suptitle(\"Random Dog and Cat Images from Training Set\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68aa58c8",
   "metadata": {},
   "source": [
    "### 2.2 Class Distribution\n",
    "Count how many images per class to check for imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b059ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.barplot(x=['Cat', 'Dog'], y=[num_cats, num_dogs], palette=['#4c72b0','#c44e52'])\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution in Training Set')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b959292",
   "metadata": {},
   "source": [
    "### 2.3 Image Statistics\n",
    "Compute basic statistics on image sizes and shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "badbf1e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "widths = []; heights = []\n",
    "for path in train_paths:\n",
    "    img = Image.open(path)\n",
    "    w, h = img.size\n",
    "    widths.append(w); heights.append(h)\n",
    "\n",
    "print(f\"Image width: min={min(widths)}, max={max(widths)}, avg={np.mean(widths):.1f}\")\n",
    "print(f\"Image height: min={min(heights)}, max={max(heights)}, avg={np.mean(heights):.1f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338c29d",
   "metadata": {},
   "source": [
    "### 3. Model Training\n",
    "\n",
    "We train two models:\n",
    "\n",
    "a) Simple CNN from scratch.\n",
    "\n",
    "b) VGG16 with Transfer Learning (pretrained on ImageNet) + fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96edefb3",
   "metadata": {},
   "source": [
    "### 3.1 Data Pipeline\n",
    "\n",
    "First, we create a TensorFlow input pipeline for training and validation. We will use two versions: one that scales pixels to [0,1] for the simple CNN, and one that applies VGG16 preprocessing (mean subtraction) for the VGG model. We resize all images to 224×224 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a3342c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "def preprocess_simple(file_path, label):\n",
    "    # Load and resize image, scale pixel values to [0,1]\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = img / 255.0\n",
    "    return img, label\n",
    "\n",
    "def preprocess_vgg(file_path, label):\n",
    "    # Load and resize image, apply VGG16 preprocessing\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = tf.image.decode_jpeg(img, channels=3)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.keras.applications.vgg16.preprocess_input(img)  # subtracts mean RGB values\n",
    "    return img, label\n",
    "\n",
    "# Create training and validation datasets for simple CNN\n",
    "train_ds_simple = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds_simple = (train_ds_simple\n",
    "                   .map(preprocess_simple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                   .shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds_simple = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "val_ds_simple = (val_ds_simple\n",
    "                 .map(preprocess_simple, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                 .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Create training and validation datasets for VGG (same split)\n",
    "train_ds_vgg = tf.data.Dataset.from_tensor_slices((train_paths, train_labels))\n",
    "train_ds_vgg = (train_ds_vgg\n",
    "                .map(preprocess_vgg, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "                .shuffle(buffer_size=1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "val_ds_vgg = tf.data.Dataset.from_tensor_slices((val_paths, val_labels))\n",
    "val_ds_vgg = (val_ds_vgg\n",
    "              .map(preprocess_vgg, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "              .batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80aaea0e",
   "metadata": {},
   "source": [
    "### 3.2 Simple CNN\n",
    "\n",
    "We build a straightforward convolutional neural network with a few Conv2D and MaxPooling layers, followed by a dense classifier. This is a binary classifier with a sigmoid output. Simpler CNNs have been shown to perform well on Cats vs Dogs classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b61a03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "model_cnn = models.Sequential([\n",
    "    layers.Conv2D(32, (3,3), activation='relu', input_shape=(*IMG_SIZE, 3)),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(64, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Conv2D(128, (3,3), activation='relu'),\n",
    "    layers.MaxPooling2D((2,2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "model_cnn.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4ee89e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_cnn = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_simple_cnn.h5', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_cnn = model_cnn.fit(\n",
    "    train_ds_simple,\n",
    "    epochs=15,\n",
    "    validation_data=val_ds_simple,\n",
    "    callbacks=callbacks_cnn\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453a56de",
   "metadata": {},
   "source": [
    "### 3.3 Fine-Tuned VGG16\n",
    "\n",
    "We now use transfer learning with Keras’ pretrained VGG16 (trained on ImageNet). First, we freeze all layers of VGG16 and add a new classifier head. Then we optionally unfreeze some top layers to fine-tune\n",
    "tensorflow.org\n",
    ". This approach often yields high accuracy even on small datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c70e060",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Load VGG16 base model without top layers, with imagenet weights\n",
    "base_model = VGG16(include_top=False, weights='imagenet', input_shape=(*IMG_SIZE,3))\n",
    "base_model.trainable = False  # freeze base\n",
    "\n",
    "# Build new top layers for our binary task\n",
    "vgg_input = base_model.input\n",
    "x = base_model.output\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "output = layers.Dense(1, activation='sigmoid')(x)\n",
    "model_vgg = models.Model(inputs=vgg_input, outputs=output)\n",
    "\n",
    "model_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model_vgg.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32251b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_vgg = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('best_vgg.h5', save_best_only=True),\n",
    "    tf.keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True)\n",
    "]\n",
    "\n",
    "history_vgg = model_vgg.fit(\n",
    "    train_ds_vgg,\n",
    "    epochs=10,\n",
    "    validation_data=val_ds_vgg,\n",
    "    callbacks=callbacks_vgg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404452c3",
   "metadata": {},
   "source": [
    "Fine-tuning: After initial training, we unfreeze some top layers of VGG16 for further training, allowing the pretrained features to adapt to our data\n",
    "tensorflow.org\n",
    ". Here we unfreeze the last convolutional block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c534e7e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze last convolutional block of VGG16\n",
    "for layer in base_model.layers[-4:]:\n",
    "    layer.trainable = True\n",
    "\n",
    "model_vgg.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "                  loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_vgg_fine = model_vgg.fit(\n",
    "    train_ds_vgg,\n",
    "    epochs=5,\n",
    "    validation_data=val_ds_vgg,\n",
    "    callbacks=callbacks_vgg\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a49ab0",
   "metadata": {},
   "source": [
    "### 3.4 Training Curves:\n",
    "Plot training/validation accuracy and loss for both models to check learning and overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1322c80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Simple CNN training curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_cnn.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('Simple CNN Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_cnn.history['loss'], label='Train Loss')\n",
    "plt.plot(history_cnn.history['val_loss'], label='Val Loss')\n",
    "plt.title('Simple CNN Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# VGG training curves\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history_vgg.history['accuracy'] + history_vgg_fine.history['accuracy'], label='Train Acc')\n",
    "plt.plot(history_vgg.history['val_accuracy'] + history_vgg_fine.history['val_accuracy'], label='Val Acc')\n",
    "plt.title('VGG16 Accuracy')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Accuracy'); plt.legend()\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history_vgg.history['loss'] + history_vgg_fine.history['loss'], label='Train Loss')\n",
    "plt.plot(history_vgg.history['val_loss'] + history_vgg_fine.history['val_loss'], label='Val Loss')\n",
    "plt.title('VGG16 Loss')\n",
    "plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4320023",
   "metadata": {},
   "source": [
    "### 4 Model Evaluation\n",
    "\n",
    "We load the best saved weights and evaluate both models on the held-out validation set. We compute accuracy, confusion matrix, precision, recall, F1-score, and plot precision-recall curves. We also show some misclassified images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c11734",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, precision_recall_curve\n",
    "\n",
    "# Load best weights\n",
    "model_cnn.load_weights('best_simple_cnn.h5')\n",
    "model_vgg.load_weights('best_vgg.h5')\n",
    "\n",
    "# Utility to evaluate model\n",
    "def evaluate_model(model, dataset, true_labels):\n",
    "    # Get predictions\n",
    "    probs = model.predict(dataset)\n",
    "    preds = (probs > 0.5).astype(int).flatten()\n",
    "    acc = np.mean(preds == true_labels)\n",
    "    cm = confusion_matrix(true_labels, preds)\n",
    "    prec = precision_score(true_labels, preds)\n",
    "    rec = recall_score(true_labels, preds)\n",
    "    f1 = f1_score(true_labels, preds)\n",
    "    # Precision-Recall curve data\n",
    "    prec_vals, rec_vals, _ = precision_recall_curve(true_labels, probs)\n",
    "    return acc, cm, prec, rec, f1, (prec_vals, rec_vals)\n",
    "\n",
    "# Prepare validation data in NumPy arrays for metrics\n",
    "val_images_simple = []\n",
    "val_images_vgg = []\n",
    "for img, lbl in val_ds_simple.unbatch():\n",
    "    val_images_simple.append(img.numpy())\n",
    "for img, lbl in val_ds_vgg.unbatch():\n",
    "    val_images_vgg.append(img.numpy())\n",
    "\n",
    "val_images_simple = np.array(val_images_simple)\n",
    "val_images_vgg = np.array(val_images_vgg)\n",
    "y_true = val_labels  # same labels\n",
    "\n",
    "# Evaluate Simple CNN\n",
    "acc_cnn, cm_cnn, prec_cnn, rec_cnn, f1_cnn, (p_cnn, r_cnn) = evaluate_model(model_cnn, val_ds_simple, y_true)\n",
    "print(f\"Simple CNN -- Accuracy: {acc_cnn:.3f}, Precision: {prec_cnn:.3f}, Recall: {rec_cnn:.3f}, F1: {f1_cnn:.3f}\")\n",
    "print(\"Confusion Matrix (rows=true, cols=predicted):\\n\", cm_cnn)\n",
    "\n",
    "# Evaluate VGG\n",
    "acc_vgg, cm_vgg, prec_vgg, rec_vgg, f1_vgg, (p_vgg, r_vgg) = evaluate_model(model_vgg, val_ds_vgg, y_true)\n",
    "print(f\"VGG16 -- Accuracy: {acc_vgg:.3f}, Precision: {prec_vgg:.3f}, Recall: {rec_vgg:.3f}, F1: {f1_vgg:.3f}\")\n",
    "print(\"Confusion Matrix (rows=true, cols=predicted):\\n\", cm_vgg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab9f66",
   "metadata": {},
   "source": [
    "### 4.1 Precision-Recall Curves\n",
    "\n",
    "Plot precision vs recall at different thresholds for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babfab46",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(r_cnn, p_cnn, label='Simple CNN')\n",
    "plt.plot(r_vgg, p_vgg, label='VGG16')\n",
    "plt.xlabel('Recall'); plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend(); plt.grid(True); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525c453a",
   "metadata": {},
   "source": [
    "### 4.2 Misclassified Examples\n",
    "Display some images that were misclassified by each model (true vs predicted label)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e2860",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def show_misclassified(model, dataset, true_labels, title):\n",
    "    # Gather all predictions\n",
    "    probs = model.predict(dataset).flatten()\n",
    "    preds = (probs > 0.5).astype(int)\n",
    "    mis_idx = np.where(preds != true_labels)[0]\n",
    "    # Show up to 6 misclassified samples\n",
    "    plt.figure(figsize=(10,4))\n",
    "    for i, idx in enumerate(random.sample(list(mis_idx), min(6, len(mis_idx)))):\n",
    "        img = mpimg.imread(val_paths[idx])\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(f\"True: {'Dog' if true_labels[idx]==1 else 'Cat'}\\nPred: {'Dog' if preds[idx]==1 else 'Cat'}\")\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(f\"Misclassified by {title}\")\n",
    "    plt.show()\n",
    "\n",
    "show_misclassified(model_cnn, val_ds_simple, y_true, title=\"Simple CNN\")\n",
    "show_misclassified(model_vgg, val_ds_vgg, y_true, title=\"VGG16\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae451126",
   "metadata": {},
   "source": [
    "### 5. Conclusions\n",
    "\n",
    "Basic Convolutional Neural Network: \n",
    "\n",
    "- This basic model can train on data comparatively faster and can achieve decent accuracy levels. As mentioned in other studies and in our case too, this basic CNN model achieved around 80 to 85 % validation accuracy on Cats vs Dogs. This has also been represented in its confusion matrix and F1 score.\n",
    "\n",
    "VGG16 Transfer Learning: \n",
    "\n",
    "- Using VGG16 that has been trained on ImageNet before and fine tuning it with further training usually results in attaining a better accuracy, especially on small data. Transfer learning improves small data generalization because it utilizes data from a larger dataset. In our results, VGG16 had better precision and recall Compared to the basic CNN. \n",
    "\n",
    "Overfitting and Regularization: \n",
    "\n",
    "- Having training and validation plots makes it easier to identify Overfitting. We implemented dropout and early stopping. If too many layers are unfrozen, the fine tuned VGG16 can overfit the small dataset. \n",
    "\n",
    "Metric Evaluation: \n",
    "\n",
    "- In order to identify the results, we used accuracy, confusion matrix, precision, recall and F1. Recall (TPR) tells us how many of the actual dogs (or cats) we managed to find correctly, while precision tells us how many of the predicted dogs are actually dogs.\n",
    "\n",
    "Advice: \n",
    "\n",
    "- The best model for practical use would be to use either the Fine-Tuned VGG16 or some other new model. This model would give you both accuracy and strength; however, if you have limited computation power, use a most likely a well-tuned basic convolutional neural network (CNN). A lot of time simple convolutional networks can work just as well for this binary problem stated by a certain research article. Ultimately it will depend on your priorities, whether you want faster processing speeds or to get highly accurate results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
